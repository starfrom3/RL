import os

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
from torch.cuda.amp import autocast, GradScaler


def confusion_matrix_show (cm,save_path='/home/qyl/CSD_data/1_CSD/DATA_clean/confusion_matrix.png'):
    # Plot confusion matrix
    plt.figure(figsize=(16, 16),dpi=60)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    # 保存图片
    plt.savefig(save_path, bbox_inches='tight', dpi=300)

    # 显示图片
    plt.show()

    # 清理当前图形
    plt.close()

from sklearn.metrics import confusion_matrix, accuracy_score


def adaptive_clip_grad(parameters, clip_factor=0.01, eps=1e-3):
    for param in parameters:
        if param.grad is None:
            continue
        param_norm = torch.norm(param)
        grad_norm = torch.norm(param.grad)
        if param_norm < eps:
            continue
        max_norm = param_norm * clip_factor
        clip_coef = max_norm / (grad_norm + eps)
        if clip_coef < 1:
            param.grad.data.mul_(clip_coef)

import torch
import numpy as np
from sklearn.metrics import f1_score, confusion_matrix
from sklearn.metrics import classification_report
class EarlyStopping:
    def __init__(self, patience=20, min_delta=0.001, mode='max'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif (self.mode == 'max' and score > self.best_score + self.min_delta) or \
             (self.mode == 'min' and score < self.best_score - self.min_delta):
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        return self.early_stop

import numpy as np
import matplotlib.pyplot as plt
import torch


#F1分数以及更新的早停机制
def plot_model_accuracies(accuracies_history,title):
    """
    绘制四个模型的准确率变化曲线，包括训练集和验证集

    Args:
        accuracies_history: 包含每个epoch的准确率记录的字典
        格式为：{
            'train': [acc1, acc2, ...],  # 训练集准确率
            'gnn': [acc1, acc2, ...],    # 验证集准确率
            'rf': [acc1, acc2, ...],
            'xgb': [acc1, acc2, ...],
            'ensemble': [acc1, acc2, ...]
        }
    """
    plt.figure(figsize=(12, 6))
    epochs = range(1, len(accuracies_history['gnn']) + 1)

    # 绘制训练集准确率曲线
    plt.plot(epochs, accuracies_history['train'], 'k--', label='Train', linewidth=2)

    # 绘制验证集上每个模型的准确率曲线
    plt.plot(epochs, accuracies_history['gnn'], 'b-', label='GNN (Val)', linewidth=2)
    plt.plot(epochs, accuracies_history['rf'], 'r-', label='RF (Val)', linewidth=2)
    plt.plot(epochs, accuracies_history['xgb'], 'g-', label='XGB (Val)', linewidth=2)
    plt.plot(epochs, accuracies_history['ensemble'], 'purple', label='Ensemble (Val)', linewidth=2)

    plt.title(f'{title}Model Accuracies over Training Epochs', fontsize=14)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=10)

    # 设置y轴范围，确保能看到细节
    plt.ylim(0.4, 1.0)

    # 添加网格
    plt.grid(True, linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.show()


def extract_raw_features(data):
    """
    只提取原始特征，不使用任何模型中间特征
    """
    # 提取分子指纹特征
    fingerprint_features = data.fingerprint.cpu().numpy()

    # 提取金属特征
    metal_features = data.metal_feats.cpu().numpy()

    # 提取所有溶剂特征
    solvent_features = np.concatenate([
        data.solvent1_feats.cpu().numpy(),
        data.solvent2_feats.cpu().numpy(),
        data.solvent3_feats.cpu().numpy(),
        data.solvent4_feats.cpu().numpy()
    ], axis=1)

    # 合并所有原始特征
    features = np.hstack([
        fingerprint_features,
        metal_features,
        solvent_features
    ])

    return features


def extract_initial_features(data_loaders):
    """
    在训练开始前提取所有数据集的原始特征
    """
    features = {'train': [], 'val': [], 'test': []}
    labels = {'train': [], 'val': [], 'test': []}
    ids = {'train': [], 'val': [], 'test': []}

    for split in ['train', 'val', 'test']:
        for data in data_loaders[split]:
            # 只提取原始特征
            feat = extract_raw_features(data)
            features[split].append(feat)
            labels[split].append(data.y.cpu().numpy())
            ids[split].extend(data.id)

            # 合并特征和标签
    for split in features:
        features[split] = np.vstack(features[split])
        labels[split] = np.concatenate(labels[split])

    return features, labels, ids


def plot_training_history(history, title=""):
    """
    绘制训练历史曲线

    Args:
        history (dict): 包含'train_acc'和'val_acc'的字典
        title (str): 图表标题前缀
    """
    plt.figure(figsize=(10, 6))
    epochs = range(1, len(history['train_acc']) + 1)

    # 绘制训练集和验证集的准确率曲线
    plt.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')
    plt.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')

    # 设置图表属性
    plt.title(f'{title}Model Accuracy over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(loc='lower right')

    # 添加网格和边框
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.gca().spines['top'].set_visible(False)
    plt.gca().spines['right'].set_visible(False)

    # Y轴范围设置为0-1
    plt.ylim(0, 1.0)

    # 保存图片
    save_path = f'/home/qyl/CSD_data/1_CSD/logs/ensemble/training_history_{title.lower().replace(" ", "_")}.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Training history plot saved as: {save_path}")

# Helper function to plot and save confusion matrices
def plot_confusion_matrix(labels, preds, class_names, title, save_path=None):
    """
    绘制并显示混淆矩阵，如果提供了 save_path，则保存图像。
    """
    if not labels or not preds:
        print(f"Skipping confusion matrix for '{title}' due to empty labels/predictions.")
        return

    cm = confusion_matrix(labels, preds)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title(title)
    plt.ylabel('Actual Label')
    plt.xlabel('Predicted Label')

    if save_path:
        plt.savefig(save_path, bbox_inches='tight')
        print(f"Confusion matrix saved to {save_path}")

    plt.show()
    plt.close()

import torch
import numpy as np
import os
from torch.cuda.amp import autocast, GradScaler
from sklearn.metrics import f1_score, confusion_matrix


def train_and_evaluate_2(
    model, optimizer, scheduler, criterion,
    data_loaders, epochs, device,
    sample_weights=None, title=""
):
    """训练和评估模型（保持原函数格式）"""

    best_val_loss = float('inf')
    best_model_state_dict = None
    best_train_results = None
    best_val_results = None

    early_stopping = EarlyStopping(patience=20, min_delta=0.001, mode='min')
    scaler = GradScaler()

    history = {'train_acc': [], 'val_acc': []}
    train_loss_list = []
    val_loss_list = []

    # =========================
    # Training & Validation
    # =========================
    for epoch in range(epochs):
        model.train()

        running_loss = 0.0
        correct_predictions = 0

        train_preds, train_labels, train_ids = [], [], []

        for i, data in enumerate(data_loaders['train']):
            data = data.to(device)
            optimizer.zero_grad()

            with autocast():
                outputs = model(data)
                labels = data.y - 1
                labels = labels.to(device)

                loss = criterion(outputs, labels.long())

                if sample_weights is not None:
                    w = sample_weights[
                        i * data.num_graphs:(i + 1) * data.num_graphs
                    ].to(device)
                    loss = (loss * w).mean()

            scaler.scale(loss).backward()
            adaptive_clip_grad(model.parameters(), clip_factor=0.01)
            scaler.step(optimizer)
            scaler.update()

            running_loss += loss.item() * data.num_graphs

            _, preds = torch.max(outputs, 1)
            preds = preds + 1

            correct_predictions += torch.sum(preds == data.y)

            train_preds.extend(preds.detach().cpu().numpy())
            train_labels.extend(data.y.detach().cpu().numpy())
            train_ids.extend(data.id)

        train_loss = running_loss / len(data_loaders['train'].dataset)
        train_acc = correct_predictions.double() / len(data_loaders['train'].dataset)
        train_f1 = f1_score(train_labels, train_preds, average='weighted')

        train_loss_list.append(train_loss)

        # ---------- Validation ----------
        val_metrics = evaluate_model_2(
            model, criterion, data_loaders['val'], device
        )

        val_loss_list.append(val_metrics['loss'])

        history['train_acc'].append(train_acc.item())
        history['val_acc'].append(val_metrics['acc'].item())

        print(f"\nEpoch {epoch + 1}/{epochs}")
        print(f"Training   - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}")
        print(f"Validation - Loss: {val_metrics['loss']:.4f}, "
              f"Acc: {val_metrics['acc']:.4f}, F1: {val_metrics['f1']:.4f}")

        # ---------- Save best ----------
        if val_metrics['loss'] < best_val_loss:
            best_val_loss = val_metrics['loss']

            # ✅ 关键修复：clone state_dict
            best_model_state_dict = {
                k: v.detach().cpu().clone()
                for k, v in model.state_dict().items()
            }

            best_train_results = (
                train_preds.copy(),
                train_labels.copy(),
                train_ids.copy()
            )

            best_val_results = (
                val_metrics['preds'].copy(),
                val_metrics['labels'].copy(),
                val_metrics['ids'].copy()
            )

        scheduler.step(val_metrics['loss'])

        if early_stopping(val_metrics['loss']):
            print("Early stopping triggered")
            break

    plot_training_history(history, title)

    # =========================
    # Test Phase（真实执行）
    # =========================
    test_metrics = {
        'loss': None,
        'acc': None,
        'f1': None,
        'labels': [],
        'preds': [],
        'ids': []
    }

    print("\nTest Results:")

    if best_model_state_dict is not None and 'test' in data_loaders:
        model.load_state_dict(best_model_state_dict)
        test_metrics = evaluate_model_2(
            model, criterion, data_loaders['test'], device
        )
    else:
        print("Skip test phase (no test loader in this fold)")

    # =========================
    # Confusion Matrices
    # =========================
    train_preds_best, train_labels_best, _ = best_train_results
    val_preds_best, val_labels_best, _ = best_val_results
    test_preds_best, test_labels_best = (
        test_metrics['preds'], test_metrics['labels']
    )

    all_labels = []
    if train_labels_best: all_labels.extend(train_labels_best)
    if val_labels_best: all_labels.extend(val_labels_best)
    if test_labels_best: all_labels.extend(test_labels_best)

    class_names = (
        [f'Class {int(i)}' for i in np.unique(all_labels)]
        if all_labels else []
    )

    datasets = {
        'Train': (train_labels_best, train_preds_best),
        'Validation': (val_labels_best, val_preds_best),
        'Test': (test_labels_best, test_preds_best)
    }

    output_dir = "/home/qyl/CSD_data/1_CSD/logs/ensemble/4.10____4/iterations6_rl"
    os.makedirs(output_dir, exist_ok=True)

    for name, (labels, preds) in datasets.items():
        if labels and preds and class_names:
            save_path = os.path.join(
                output_dir,
                f'confusion_matrix_{name.lower()}_{title}.png'
            )
            plot_confusion_matrix(
                labels, preds, class_names,
                title=f"{name} Set Confusion Matrix\n({title})",
                save_path=save_path
            )

    return {
        "model": model,
        "best_train_results": best_train_results,
        "best_test_results": {
            "loss": test_metrics["loss"],
            "preds": test_metrics['preds'],
            "labels": test_metrics['labels'],
            "ids": test_metrics['ids']
        },
        "train_loss_history": train_loss_list,
        "val_loss_history": val_loss_list
    }


def evaluate_model_2(model, criterion, data_loader, device):
    """评估模型性能（保持原函数格式）"""

    metrics = {
        'loss': 0.0,
        'acc': 0.0,
        'f1': 0.0,
        'preds': [],
        'labels': [],
        'ids': [],
        'details': []
    }

    if not data_loader or len(data_loader.dataset) == 0:
        print("[Warning] Empty evaluation dataset")
        return metrics

    model.eval()
    running_loss = 0.0
    correct_predictions = 0

    with torch.no_grad():
        for data in data_loader:
            data = data.to(device)

            with autocast():
                outputs = model(data)
                labels = data.y - 1
                loss = criterion(outputs, labels.long())

            _, preds = torch.max(outputs, 1)
            preds = preds + 1

            running_loss += loss.item() * data.num_graphs
            correct_predictions += torch.sum(preds == data.y)

            metrics['preds'].extend(preds.cpu().numpy())
            metrics['labels'].extend(data.y.cpu().numpy())
            metrics['ids'].extend(data.id.cpu().numpy())

    dataset_size = len(data_loader.dataset)
    if dataset_size > 0:
        metrics['loss'] = running_loss / dataset_size
        metrics['acc'] = correct_predictions.double() / dataset_size
        metrics['f1'] = f1_score(
            metrics['labels'], metrics['preds'], average='weighted'
        )

    return metrics
