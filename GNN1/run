import argparse
import copy
import hashlib
import json
import logging
import os
import pickle
import random
import tempfile
import warnings
from collections import Counter
from datetime import datetime
from typing import List, Dict, Tuple, Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from rdkit import RDLogger
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import GroupShuffleSplit, KFold
from torch.utils.data import Subset
from torch_geometric.data import DataLoader
from torch_geometric.loader import DataLoader

from model_3 import GNN
from process_data_predict import MolecularDataProcessor, process_data_with_fingerprints
from toolll import augment_data
from train_3 import train_and_evaluate
from train_3_2 import train_and_evaluate_2, evaluate_model_2

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
lg = RDLogger.logger()
lg.setLevel(RDLogger.CRITICAL)
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
temp_dir = '/home/qyl/tmp2'
os.makedirs(temp_dir, exist_ok=True)
os.environ['TMPDIR'] = temp_dir
tempfile.tempdir = temp_dir
print("å½“å‰ä¸´æ—¶ç›®å½•:", tempfile.gettempdir())
print("ç¯å¢ƒå˜é‡TMPDIR:", os.environ.get('TMPDIR'))


class DataCacheManager:
    """
    æ•°æ®ç¼“å­˜ç®¡ç†å™¨ - ä¿å­˜å’ŒåŠ è½½å¤„ç†å¥½çš„æ•°æ®
    """

    def __init__(self, cache_dir: str = "./data_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def _generate_cache_key(self, **kwargs) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®å€¼ï¼ŒåŸºäºæ•°æ®å¤„ç†å‚æ•°
        """
        # å°†å‚æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ’åº
        param_str = json.dumps(kwargs, sort_keys=True, default=str)
        # ç”ŸæˆMD5å“ˆå¸Œ
        cache_key = hashlib.md5(param_str.encode()).hexdigest()[:16]
        return cache_key

    def _get_cache_info_path(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜ä¿¡æ¯æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}_info.json")

    def _get_cache_data_path(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜æ•°æ®æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}_data.pkl")

    def save_processed_data(self,
                            train_data: List,
                            val_data: List,
                            test_data: List,
                            data_processor,
                            cache_params: Dict) -> str:
        """
        ä¿å­˜å¤„ç†å¥½çš„æ•°æ®

        Args:
            train_data: è®­ç»ƒæ•°æ®
            val_data: éªŒè¯æ•°æ®
            test_data: æµ‹è¯•æ•°æ®
            data_processor: æ•°æ®å¤„ç†å™¨ï¼ˆåŒ…å«PCAç­‰ï¼‰
            cache_params: ç¼“å­˜å‚æ•°ï¼ˆç”¨äºç”Ÿæˆç¼“å­˜é”®ï¼‰

        Returns:
            str: ç¼“å­˜é”®
        """
        cache_key = self._generate_cache_key(**cache_params)

        # ä¿å­˜æ•°æ®
        data_dict = {
            'train_data': train_data,
            'val_data': val_data,
            'test_data': test_data,
            'data_processor': data_processor,
            'cache_params': cache_params,
            'save_time': datetime.now().isoformat(),
            'data_sizes': {
                'train': len(train_data),
                'val': len(val_data),
                'test': len(test_data)
            }
        }

        data_path = self._get_cache_data_path(cache_key)
        with open(data_path, 'wb') as f:
            pickle.dump(data_dict, f)

        # ä¿å­˜ç¼“å­˜ä¿¡æ¯
        cache_info = {
            'cache_key': cache_key,
            'cache_params': cache_params,
            'save_time': datetime.now().isoformat(),
            'data_sizes': data_dict['data_sizes'],
            'data_path': data_path
        }

        info_path = self._get_cache_info_path(cache_key)
        with open(info_path, 'w') as f:
            json.dump(cache_info, f, indent=2)

        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°ç¼“å­˜: {cache_key}")
        print(f"   è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
        print(f"   ç¼“å­˜æ–‡ä»¶: {data_path}")

        return cache_key

    def load_processed_data(self, cache_key: str) -> Optional[Dict]:
        """
        åŠ è½½å¤„ç†å¥½çš„æ•°æ®

        Args:
            cache_key: ç¼“å­˜é”®

        Returns:
            Dict: åŒ…å«æ•°æ®å’Œå¤„ç†å™¨çš„å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        data_path = self._get_cache_data_path(cache_key)

        if not os.path.exists(data_path):
            print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_key}")
            return None

        try:
            with open(data_path, 'rb') as f:
                data_dict = pickle.load(f)

            print(f"âœ… ä»ç¼“å­˜åŠ è½½æ•°æ®: {cache_key}")
            print(f"   è®­ç»ƒé›†: {len(data_dict['train_data'])} æ ·æœ¬")
            print(f"   éªŒè¯é›†: {len(data_dict['val_data'])} æ ·æœ¬")
            print(f"   æµ‹è¯•é›†: {len(data_dict['test_data'])} æ ·æœ¬")
            print(f"   ä¿å­˜æ—¶é—´: {data_dict['save_time']}")

            return data_dict

        except Exception as e:
            print(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None

    def check_cache_exists(self, **cache_params) -> Optional[str]:
        """
        æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨

        Args:
            **cache_params: ç¼“å­˜å‚æ•°

        Returns:
            str: å¦‚æœå­˜åœ¨è¿”å›ç¼“å­˜é”®ï¼Œå¦åˆ™è¿”å›None
        """
        cache_key = self._generate_cache_key(**cache_params)
        data_path = self._get_cache_data_path(cache_key)

        if os.path.exists(data_path):
            return cache_key
        return None

    def list_cached_data(self) -> List[Dict]:
        """
        åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„æ•°æ®

        Returns:
            List[Dict]: ç¼“å­˜ä¿¡æ¯åˆ—è¡¨
        """
        cached_data = []

        for filename in os.listdir(self.cache_dir):
            if filename.endswith('_info.json'):
                info_path = os.path.join(self.cache_dir, filename)
                try:
                    with open(info_path, 'r') as f:
                        cache_info = json.load(f)
                    cached_data.append(cache_info)
                except:
                    continue

        # æŒ‰æ—¶é—´æ’åº
        cached_data.sort(key=lambda x: x['save_time'], reverse=True)
        return cached_data

    def delete_cache(self, cache_key: str) -> bool:
        """
        åˆ é™¤æŒ‡å®šçš„ç¼“å­˜

        Args:
            cache_key: ç¼“å­˜é”®

        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        data_path = self._get_cache_data_path(cache_key)
        info_path = self._get_cache_info_path(cache_key)

        success = True
        if os.path.exists(data_path):
            try:
                os.remove(data_path)
                print(f"âœ… åˆ é™¤ç¼“å­˜æ•°æ®: {data_path}")
            except Exception as e:
                print(f"âŒ åˆ é™¤ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
                success = False

        if os.path.exists(info_path):
            try:
                os.remove(info_path)
                print(f"âœ… åˆ é™¤ç¼“å­˜ä¿¡æ¯: {info_path}")
            except Exception as e:
                print(f"âŒ åˆ é™¤ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
                success = False

        return success

    def get_cache_size(self) -> str:
        """
        è·å–ç¼“å­˜ç›®å½•å¤§å°

        Returns:
            str: ç¼“å­˜å¤§å°ï¼ˆäººç±»å¯è¯»æ ¼å¼ï¼‰
        """
        total_size = 0
        for filename in os.listdir(self.cache_dir):
            filepath = os.path.join(self.cache_dir, filename)
            if os.path.isfile(filepath):
                total_size += os.path.getsize(filepath)

        # è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼
        for unit in ['B', 'KB', 'MB', 'GB']:
            if total_size < 1024.0:
                return f"{total_size:.2f} {unit}"
            total_size /= 1024.0
        return f"{total_size:.2f} TB"


def create_data_loaders_with_cache(
        args,
        df: pd.DataFrame,
        metal_properties_sheet: pd.DataFrame,
        solvent_properties_sheet: pd.DataFrame,
        cache_manager: DataCacheManager,
        force_reload: bool = False
) -> Tuple[Dict, DataCacheManager,Dict]:
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå¸¦ç¼“å­˜åŠŸèƒ½ï¼‰

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        df: ä¸»æ•°æ®æ¡†
        metal_properties_sheet: é‡‘å±å±æ€§æ•°æ®
        solvent_properties_sheet: æº¶å‰‚å±æ€§æ•°æ®
        cache_manager: ç¼“å­˜ç®¡ç†å™¨
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†æ•°æ®

    Returns:
        Tuple[Dict, DataCacheManager]: æ•°æ®åŠ è½½å™¨å­—å…¸å’Œç¼“å­˜ç®¡ç†å™¨
    """

    # å®šä¹‰ç¼“å­˜å‚æ•°ï¼ˆç”¨äºç”Ÿæˆç¼“å­˜é”®ï¼‰
    cache_params = {
        'data_path': args.data_path,
        'sheet': args.sheet,
        'test_size': 0.2,
        'random_state': 42,  # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­
        'n_components': 50,
        'source_batch_size': args.source_batch_size,
        'ori_arg': args.ori_arg,
        # å¯ä»¥æ·»åŠ å…¶ä»–å½±å“æ•°æ®å¤„ç†çš„å‚æ•°
        'data_hash': hashlib.md5(df.to_string().encode()).hexdigest()[:16]  # æ•°æ®å†…å®¹å“ˆå¸Œ
    }

    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
    cache_key = None
    if not force_reload:
        cache_key = cache_manager.check_cache_exists(**cache_params)

    if cache_key:
        print("ğŸ”„ å‘ç°ç¼“å­˜æ•°æ®ï¼Œæ­£åœ¨åŠ è½½...")

        # ä»ç¼“å­˜åŠ è½½æ•°æ®
        cached_data = cache_manager.load_processed_data(cache_key)

        if cached_data:
            # é‡æ–°åˆ›å»ºDataLoader
            train_data = cached_data['train_data']
            val_data = cached_data['val_data']
            test_data = cached_data['test_data']
            data_processor = cached_data['data_processor']

            # æ•°æ®å¢å¼ºï¼ˆå¦‚æœéœ€è¦ï¼‰
            if args.ori_arg:
                print("ğŸ”„ åº”ç”¨æ•°æ®å¢å¼º...")
                augmented_train_data = flexible_augment_data(
                    train_data,
                    augmentation_probability=1,
                    max_augmentations=1
                )
                train_loader = DataLoader(augmented_train_data, batch_size=args.source_batch_size, shuffle=True)
            else:
                train_loader = DataLoader(train_data, batch_size=args.source_batch_size, shuffle=True)

            val_loader = DataLoader(val_data, batch_size=args.source_batch_size, shuffle=False)
            test_loader = DataLoader(test_data, batch_size=args.source_batch_size, shuffle=False)

            origin_data={
                "train": train_data,
                "val": val_data,
                "test": test_data,
                "data_processor": data_processor  # ä¹Ÿè¿”å›æ•°æ®å¤„ç†å™¨
            }

            data_loaders = {
                "train": train_loader,
                "val": val_loader,
                "test": test_loader,
                "data_processor": data_processor  # ä¹Ÿè¿”å›æ•°æ®å¤„ç†å™¨
            }

            print("âœ… ä»ç¼“å­˜æˆåŠŸåŠ è½½æ•°æ®ï¼")
            return data_loaders, cache_manager,origin_data

    # ç¼“å­˜ä¸å­˜åœ¨æˆ–å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œæ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†
    print("ğŸ”„ ç¼“å­˜ä¸å­˜åœ¨æˆ–å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œå¼€å§‹å¤„ç†æ•°æ®...")

    # 2. æ•°æ®é›†åˆ’åˆ†å’Œç‰¹å¾æå–
    data_processor = MolecularDataProcessor(n_components=50)
    random2 = 42

    # åˆ’åˆ†æ•°æ®é›†å¹¶æå–ç‰¹å¾
    (train_df, val_df, test_df), (train_fingerprints_transformed,
                                  val_fingerprints_transformed,
                                  test_fingerprints_transformed) = prepare_datasets(
        df, data_processor, test_size=0.2, random_state=random2
    )

    # ä¿å­˜PCAå’ŒStandardScaleræ¨¡å‹
    if args.pca_model_path:
        os.makedirs(os.path.dirname(f"{args.pca_model_path}/{args.time}_{args.record}_{args.count}.pkl"), exist_ok=True)
        data_processor.save_pca_model(f"{args.pca_model_path}/{args.time}_{args.record}_{args.count}.pkl")

    # 3. å‡†å¤‡GNNæ•°æ®
    print("ğŸ”„ å¤„ç†GNNæ•°æ®...")
    train_data = process_data_with_fingerprints(
        train_df, metal_properties_sheet, solvent_properties_sheet,
        train_fingerprints_transformed, augment=False
    )
    val_data = process_data_with_fingerprints(
        val_df, metal_properties_sheet, solvent_properties_sheet,
        val_fingerprints_transformed, augment=False
    )
    test_data = process_data_with_fingerprints(
        test_df, metal_properties_sheet, solvent_properties_sheet,
        test_fingerprints_transformed, augment=False
    )

    # ä¿å­˜åˆ°ç¼“å­˜
    print("ğŸ’¾ ä¿å­˜æ•°æ®åˆ°ç¼“å­˜...")
    cache_key = cache_manager.save_processed_data(
        train_data, val_data, test_data, data_processor, cache_params
    )

    # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    if args.ori_arg:
        print("ğŸ”„ åº”ç”¨æ•°æ®å¢å¼º...")
        augmented_train_data = flexible_augment_data(
            train_data,
            augmentation_probability=1,
            max_augmentations=1
        )
        train_loader = DataLoader(augmented_train_data, batch_size=args.source_batch_size, shuffle=True)
    else:
        train_loader = DataLoader(train_data, batch_size=args.source_batch_size, shuffle=True)

    val_loader = DataLoader(val_data, batch_size=args.source_batch_size, shuffle=False)
    test_loader = DataLoader(test_data, batch_size=args.source_batch_size, shuffle=False)

    origin_data = {
        "train": train_data,
        "val": val_data,
        "test": test_data,
        "data_processor": data_processor  # ä¹Ÿè¿”å›æ•°æ®å¤„ç†å™¨
    }

    data_loaders = {
        "train": train_loader,
        "val": val_loader,
        "test": test_loader,
        "data_processor": data_processor
    }

    print("âœ… æ•°æ®å¤„ç†å®Œæˆå¹¶å·²ç¼“å­˜ï¼")
    return data_loaders, cache_manager,origin_data


def manage_cache_interactive(cache_manager: DataCacheManager):
    """
    äº¤äº’å¼ç®¡ç†ç¼“å­˜
    """
    print("\nğŸ“‚ ç¼“å­˜ç®¡ç†å™¨")
    print("=" * 50)

    # åˆ—å‡ºæ‰€æœ‰ç¼“å­˜
    cached_data = cache_manager.list_cached_data()

    if not cached_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¼“å­˜æ•°æ®")
        return

    print(f"ğŸ“Š ç¼“å­˜æ€»å¤§å°: {cache_manager.get_cache_size()}")
    print(f"ğŸ“ ç¼“å­˜æ•°é‡: {len(cached_data)}")
    print("\nç¼“å­˜åˆ—è¡¨:")

    for i, cache_info in enumerate(cached_data):
        print(f"\n{i + 1}. ç¼“å­˜é”®: {cache_info['cache_key']}")
        print(f"   ä¿å­˜æ—¶é—´: {cache_info['save_time']}")
        print(f"   æ•°æ®è§„æ¨¡: è®­ç»ƒ={cache_info['data_sizes']['train']}, "
              f"éªŒè¯={cache_info['data_sizes']['val']}, "
              f"æµ‹è¯•={cache_info['data_sizes']['test']}")

        # æ˜¾ç¤ºä¸»è¦å‚æ•°
        if 'cache_params' in cache_info:
            params = cache_info['cache_params']
            print(f"   ä¸»è¦å‚æ•°: batch_size={params.get('source_batch_size', 'N/A')}, "
                  f"ori_arg={params.get('ori_arg', 'N/A')}")

    # æä¾›åˆ é™¤é€‰é¡¹
    print("\né€‰é¡¹:")
    print("0. è¿”å›")
    print("1. åˆ é™¤æŒ‡å®šç¼“å­˜")
    print("2. æ¸…ç©ºæ‰€æœ‰ç¼“å­˜")

    try:
        choice = int(input("\nè¯·é€‰æ‹©æ“ä½œ: "))

        if choice == 0:
            return
        elif choice == 1:
            cache_num = int(input("è¯·è¾“å…¥è¦åˆ é™¤çš„ç¼“å­˜ç¼–å·: ")) - 1
            if 0 <= cache_num < len(cached_data):
                cache_key = cached_data[cache_num]['cache_key']
                if cache_manager.delete_cache(cache_key):
                    print("âœ… ç¼“å­˜åˆ é™¤æˆåŠŸ")
                else:
                    print("âŒ ç¼“å­˜åˆ é™¤å¤±è´¥")
            else:
                print("âŒ æ— æ•ˆçš„ç¼“å­˜ç¼–å·")
        elif choice == 2:
            confirm = input("ç¡®è®¤åˆ é™¤æ‰€æœ‰ç¼“å­˜? (y/N): ")
            if confirm.lower() == 'y':
                for cache_info in cached_data:
                    cache_manager.delete_cache(cache_info['cache_key'])
                print("âœ… æ‰€æœ‰ç¼“å­˜å·²åˆ é™¤")

    except (ValueError, KeyboardInterrupt):
        print("\næ“ä½œå·²å–æ¶ˆ")

class EarlyStopping:
    def __init__(self, patience=10, min_delta=0, verbose=False, delta=None, scheduler=None):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.delta = delta if delta is not None else min_delta
        self.scheduler = scheduler
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score - self.min_delta:
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f'Early stopping triggered after {self.patience} epochs without improvement.')

    def step(self):
        if self.scheduler:
            self.scheduler.step()


#ä¿å­˜æ–‡ä»¶
def save_results_to_excel(train_results, val_results, test_results, output_dir, filename):
    def to_cpu_numpy(tensor):
        if isinstance(tensor, torch.Tensor):
            return tensor.cpu().detach().numpy()
        elif isinstance(tensor, np.ndarray):
            return tensor
        elif isinstance(tensor, list):
            return np.array([to_cpu_numpy(t) for t in tensor])
        return np.array(tensor)

    train_preds, train_labels, train_ids = map(to_cpu_numpy, train_results)
    val_preds, val_labels, val_ids = map(to_cpu_numpy, val_results)
    test_preds, test_labels, test_ids = map(to_cpu_numpy, test_results)

    df_train = pd.DataFrame({
        'ID': train_ids,
        'Actual': train_labels,
        'Predicted': train_preds,
        'Set': 'Train'
    })

    df_val = pd.DataFrame({
        'ID': val_ids,
        'Actual': val_labels,
        'Predicted': val_preds,
        'Set': 'Validation'
    })

    df_test = pd.DataFrame({
        'ID': test_ids,
        'Actual': test_labels,
        'Predicted': test_preds,
        'Set': 'Test'
    })

    df = pd.concat([df_train, df_val, df_test], ignore_index=True)
    df = df.sort_values(['Set', 'ID'])

    output_path = os.path.join(output_dir, filename)
    df.to_excel(output_path, index=False)
    print(f"Results saved to {output_path}")


#éšæœºåˆ¤æ–­æ•°æ®æ˜¯å¦å¢å¹¿
def flexible_augment_data(data_list, augmentation_probability=0.5, max_augmentations=1):
    augmented_data_list = []
    for data in data_list:
        augmented_data_list.append(data)

        # éšæœºå†³å®šæ˜¯å¦å¯¹è¿™ä¸ªæ•°æ®ç‚¹è¿›è¡Œå¢å¼º
        if random.random() < augmentation_probability:
            num_augmentations = random.randint(1, max_augmentations)
            for _ in range(num_augmentations):
                augmented_data_list.append(augment_data(data))

    return augmented_data_list

def calculate_metrics(preds, labels, ids):
    accuracy = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average='weighted')
    return {"accuracy": accuracy, "f1": f1}

# è®¡ç®—å¹³å‡äº¤å‰éªŒè¯ç»“æœ
def average_metrics(outputs):
    avg_accuracy = np.mean([calculate_metrics(*output)['accuracy'] for output in outputs])
    avg_f1 = np.mean([calculate_metrics(*output)['f1'] for output in outputs])
    return {"accuracy": avg_accuracy, "f1": avg_f1}



def assign_group_id_by_solvents(df):
    """
    åŸºäº [SMILES, Solvent1, Solvent2, Solvent3, Solvent4] åˆ—è¿›è¡Œåˆ†ç»„ï¼Œ
    è‹¥å¤šæ¡è®°å½•åœ¨è¿™äº›åˆ—ä¸Šçš„å–å€¼å®Œå…¨ç›¸åŒï¼Œåˆ™åˆ†é…ç›¸åŒçš„ group_idã€‚
    """
    group_cols = ["SMILES", "Solvent1", "Solvent2", "Solvent3", "Solvent4"]
    df["group_id"] = df.groupby(group_cols, dropna=False).ngroup()
    return df

def group_based_split(df, test_size=0.2, random_state=42):
    """
    ä½¿ç”¨ GroupShuffleSplit å°† df åˆ†ä¸º train_df ä¸ test_dfï¼Œæ¯”ä¾‹ç”± test_size å†³å®šã€‚
    ä¿è¯åŒä¸€ group_id çš„æ•°æ®ä¸ä¼šåˆ†æ‹†åˆ°ä¸åŒé›†ã€‚
    """
    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)

    X = df.index  # ä»…ç”¨ä½œå ä½
    y = df.index  # ä»…ç”¨ä½œå ä½
    groups = df["group_id"].values

    train_idx, test_idx = next(gss.split(X, y, groups=groups))

    df_train = df.iloc[train_idx].copy()
    df_test = df.iloc[test_idx].copy()

    return df_train, df_test

def prepare_datasets(df, data_processor, test_size=0.2, random_state=42, is_training=True):
    """
    å‡†å¤‡è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†ï¼š
      1) æ ¹æ® [SMILES, Solvent1, Solvent2, Solvent3, Solvent4] è¿›è¡Œåˆ†ç»„ï¼›
      2) ä½¿ç”¨ group_based_split å…ˆæ‹†åˆ†å‡º (train_df, temp_df)ï¼Œå†æ‹†åˆ†å‡º (val_df, test_df)ï¼›
      3) è®¡ç®—å¹¶(å¯é€‰)PCAé™ç»´åˆ†å­æŒ‡çº¹ã€‚
    """

    # (a) å…ˆç»™æ•°æ®æ‰“ä¸Šåˆ†ç»„æ ‡ç­¾
    df = assign_group_id_by_solvents(df)

    # (b) ç¬¬ä¸€æ¬¡æ‹†åˆ†: è®­ç»ƒé›† & ä¸´æ—¶é›† (temp)
    #    ä¾‹å¦‚ï¼Œè¿™é‡Œ test_size=0.2ï¼Œè¡¨ç¤ºå…ˆæŠŠ 20% æ•°æ®ç•™ä½œ tempï¼Œå…¶ä½™ 80% ä½œä¸ºè®­ç»ƒé›†
    train_df, temp_df = group_based_split(df, test_size=test_size, random_state=random_state)

    # (c) ç¬¬äºŒæ¬¡æ‹†åˆ†: éªŒè¯é›† & æµ‹è¯•é›†ï¼ˆæ­¤å¤„ç®€å• 1:1 æ‹†åˆ† temp_dfï¼Œå³ val 10%, test 10%ï¼‰
    val_df, test_df = group_based_split(temp_df, test_size=0.5, random_state=random_state)
    # ç¡®ä¿æ‹†åˆ†åçš„æ•°æ®é›†åŒ…å« group_id
    for subset in [train_df, val_df, test_df]:
        assert "group_id" in subset.columns, f"æ‹†åˆ†åçš„æ•°æ®é›† {subset} ç¼ºå°‘ group_id åˆ—ï¼"
    # 2. è®¡ç®—åˆ†å­æŒ‡çº¹
    train_fingerprints_raw = data_processor.compute_fingerprints(train_df)
    val_fingerprints_raw = data_processor.compute_fingerprints(val_df)
    test_fingerprints_raw = data_processor.compute_fingerprints(test_df)

    # 3. PCAå¤„ç†
    if is_training:
        # è®­ç»ƒæ¨¡å¼: æ‹Ÿåˆå¹¶è½¬æ¢è®­ç»ƒé›†, ä»…è½¬æ¢éªŒè¯é›†å’Œæµ‹è¯•é›†
        train_fingerprints = data_processor.fit_transform_fingerprints(train_fingerprints_raw)
        val_fingerprints = data_processor.transform_fingerprints(val_fingerprints_raw)
        test_fingerprints = data_processor.transform_fingerprints(test_fingerprints_raw)
    else:
        # é¢„æµ‹/æ¨ç†æ¨¡å¼: ä½¿ç”¨å·²æœ‰PCAæ¨¡å‹è½¬æ¢
        train_fingerprints = data_processor.transform_fingerprints(train_fingerprints_raw)
        val_fingerprints = data_processor.transform_fingerprints(val_fingerprints_raw)
        test_fingerprints = data_processor.transform_fingerprints(test_fingerprints_raw)

    # 4. è¿”å›æ•°æ®é›†å’Œå¯¹åº”æŒ‡çº¹
    return (train_df, val_df, test_df), (train_fingerprints, val_fingerprints, test_fingerprints)



def _safe_model_copy(model):
    """
    Safely create a deep copy of a PyTorch model.

    Args:
        model (torch.nn.Module): Original model to copy

    Returns:
        torch.nn.Module: A deep copy of the model
    """
    if model is None:
        return None

    try:
        # Deep copy the model's state dictionary
        new_model = copy.deepcopy(model)

        # Ensure the model is in the same mode as the original
        new_model.train(model.training)

        return new_model
    except Exception as e:
        logging.error(f"Model copy failed: {e}")
        return None


def _create_run_statistic(run_idx, best_loss, seed):
    """
    Create a dictionary of statistics for a single run.

    Args:
        run_idx (int): Current run index
        best_loss (float): Best loss achieved in the run
        seed (int): Random seed used for the run

    Returns:
        dict: Run statistics
    """
    return {
        'run': run_idx + 1,
        'best_loss': best_loss,
        'seed': seed,
        'timestamp': datetime.now().isoformat()
    }


def _evaluate_final_model(global_best_model, test_loader, device, logger=None):
    """
    Evaluate the final (global best) model on the test set.

    Args:
        global_best_model (torch.nn.Module): Best model from cross-validation
        test_loader (DataLoader): Test data loader
        device (torch.device): Computing device
        logger (logging.Logger, optional): Logger for recording evaluation results

    Returns:
        dict: Test set evaluation metrics
    """
    if global_best_model is None:
        if logger:
            logger.error("No valid global best model for final evaluation")
        return {'error': 'No valid model'}

    try:
        # Ensure model is in evaluation mode
        global_best_model.eval()

        # Prepare metrics tracking
        total_loss = 0.0
        correct_predictions = 0
        total_samples = 0
        all_predictions = []
        all_targets = []

        # Disable gradient computation
        with torch.no_grad():
            for batch in test_loader:
                # Unpack batch (adjust according to your data structure)
                inputs, targets = batch
                inputs, targets = inputs.to(device), targets.to(device)

                # Forward pass
                outputs = global_best_model(inputs)

                # Compute loss
                loss = global_best_model.compute_loss(outputs, targets)
                total_loss += loss.item()

                # Compute predictions
                _, predicted = torch.max(outputs, 1)
                total_samples += targets.size(0)
                correct_predictions += (predicted == targets).sum().item()

                # Store for more detailed metrics
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())

        # Compute metrics
        accuracy = correct_predictions / total_samples
        avg_loss = total_loss / len(test_loader)

        # Compute F1 Score
        f1 = f1_score(all_targets, all_predictions, average='weighted')

        # Logging
        test_metrics = {
            'loss': avg_loss,
            'acc': accuracy,
            'f1': f1,
            'total_samples': total_samples,
            'correct_predictions': correct_predictions
        }

        if logger:
            logger.info("\n=== Final Test Evaluation ===")
            logger.info(f"Loss: {avg_loss:.4f}")
            logger.info(f"Accuracy: {accuracy:.4f}")
            logger.info(f"F1 Score: {f1:.4f}")

        return test_metrics

    except Exception as e:
        if logger:
            logger.error(f"Test evaluation failed: {e}")
        return {'error': str(e)}


def _save_global_best_model(global_best_model, save_dir):
    """
    Save the global best model to a specified directory.

    Args:
        global_best_model (torch.nn.Module): Best model from cross-validation
        save_dir (str): Directory to save the model

    Returns:
        str: Path to the saved model, or None if saving failed
    """
    if global_best_model is None:
        logging.warning("No model to save")
        return None

    try:
        # Ensure save directory exists
        os.makedirs(save_dir, exist_ok=True)

        # Construct save path
        save_path = os.path.join(save_dir, "global_best_model.pth")

        # Save model state dictionary
        torch.save(global_best_model.state_dict(), save_path)

        logging.info(f"Global best model saved to {save_path}")
        return save_path

    except Exception as e:
        logging.error(f"Failed to save global best model: {e}")
        return None


def _get_model_details(model):
    """
    Extract detailed information about the model.

    Args:
        model (torch.nn.Module): PyTorch model

    Returns:
        dict: Model details and summary
    """
    if model is None:
        return {'error': 'No model provided'}

    try:
        # Total number of parameters
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        # Model architecture summary
        model_summary = {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'parameter_ratio': trainable_params / total_params,
            'model_type': type(model).__name__,
        }

        # Optional: Layer-wise parameter count
        if hasattr(model, 'state_dict'):
            layer_params = {
                name: params.numel()
                for name, params in model.state_dict().items()
            }
            model_summary['layer_parameters'] = layer_params

        return model_summary

    except Exception as e:
        logging.error(f"Failed to get model details: {e}")
        return {'error': str(e)}

def _create_fold_record(run_idx, fold_idx, train_idx, val_idx, training_result, val_metrics):
    """
    Create a comprehensive record for a single fold.

    Args:
        run_idx (int): Current run index
        fold_idx (int): Current fold index
        train_idx (list): Training indices
        val_idx (list): Validation indices
        training_result (dict): Training result from train_and_evaluate_2
        val_metrics (dict): Validation metrics

    Returns:
        dict: Detailed fold record
    """
    return {
        'run': run_idx + 1,
        'fold': fold_idx + 1,
        'train_size': len(train_idx),
        'val_size': len(val_idx),
        'train_metrics': {
            'loss': training_result.get('train_loss_history', [])[-1] if training_result.get(
                'train_loss_history') else None,
            # Add other train metrics as needed
        },
        'val_metrics': {
            'loss': val_metrics.get('loss'),
            'accuracy': val_metrics.get('acc'),
            'f1_score': val_metrics.get('f1'),
        },
        'best_train_results': training_result.get('best_train_results'),
        'best_val_results': training_result.get('best_val_results'),
        'training_epochs': len(training_result.get('train_loss_history', [])),
        'early_stopped': False  # You might want to track this explicitly
    }


def kf_cross_validate_transfer_learning(
        model_template, full_dataset, test_loader, args, device, num_runs=10, n_splits=5
):
    # Logging setup
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s: %(message)s',
        handlers=[
            logging.FileHandler(os.path.join(args.fine_save, 'cross_validation_log.txt')),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)

    # Global performance tracking
    global_best_loss = float('inf')
    global_best_model = None
    all_fold_results = []
    run_statistics = []

    # Performance tracking containers
    run_performance_history = {
        'train_losses': [],
        'val_losses': [],
        'accuracies': [],
        'f1_scores': []
    }

    for run_idx in range(num_runs):
        # Reproducibility
        run_seed = random.randint(0, 0xffff_ffff)
        set_all_seeds(run_seed)

        logger.info(f"\n{'#' * 30} Run {run_idx + 1}/{num_runs} (Seed: {run_seed}) {'#' * 30}")

        # Run-level variables
        run_best_loss = float('inf')
        run_best_model = None
        fold_records = []

        # K-Fold Cross-Validation
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=run_seed)
        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(full_dataset)):
            logger.info(f"\n{'=' * 20} Fold {fold_idx + 1}/{n_splits} {'=' * 20}")

            # Safe model initialization
            try:
                model = _safe_model_initialization(
                    model_template, device, run_idx, fold_idx
                )

                # åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
                optimizer = torch.optim.AdamW(
                    filter(lambda p: p.requires_grad, model.parameters()),
                    lr=args.lr,
                    weight_decay=args.weight_decay
                )
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, mode='min', factor=0.5, patience=15
                )

            except Exception as init_error:
                logger.error(f"Model initialization failed: {init_error}")
                continue

            # Data Loaders
            train_loader, val_loader = _create_data_loaders(
                full_dataset, train_idx, val_idx, args
            )

            # Training Pipeline
            try:
                # ä¼ é€’ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
                training_result = train_and_evaluate_2(
                    model=model,
                    optimizer=optimizer,
                    scheduler=scheduler,
                    criterion=lambda pred, target: model.compute_loss(pred, target),
                    data_loaders={"train": train_loader, "val": val_loader},
                    epochs=args.epochs,
                    device=device,
                    title=f"Run{run_idx + 1}_Fold{fold_idx + 1}_"
                )

                # Extract trained model from training result
                trained_model = training_result.get('model')
                if trained_model is None:
                    logger.error("No trained model found in training result")
                    continue

                # Evaluation
                val_metrics = _evaluate_fold(
                    trained_model, val_loader, device
                )

                # When tracking the best model in each fold
                if val_metrics['loss'] < run_best_loss:
                    run_best_loss = val_metrics['loss']

                    # Create a safe copy of the model template
                    run_best_model = _safe_model_copy(model_template)

                    # Load the state dict from the trained model in training_result
                    if training_result and 'model' in training_result:
                        run_best_model.load_state_dict(training_result['model'].state_dict())

                    run_best_model = run_best_model.to(device)

                # Create fold record
                fold_record = _create_fold_record(
                    run_idx,
                    fold_idx,
                    train_idx,
                    val_idx,
                    training_result,  # Pass the entire training result
                    val_metrics
                )

                # Add the fold record to fold_records
                fold_records.append(fold_record)

            except Exception as train_error:
                logger.error(f"Training failed in Run {run_idx + 1}, Fold {fold_idx + 1}: {train_error}")
                continue

        # Save run's best model
        _save_run_best_model(run_best_model, args.fine_save, run_idx)

        # Update global best model
        if run_best_loss < global_best_loss and run_best_model is not None:
            global_best_loss = run_best_loss
            global_best_model = _safe_model_copy(run_best_model)
            logger.info(f"\nğŸ”¥ Global Best Updated! Run {run_idx + 1} Loss: {global_best_loss:.4f}")

        # Aggregate run statistics
        run_performance_history['train_losses'].append(run_best_loss)
        run_statistics.append(_create_run_statistic(run_idx, run_best_loss, run_seed))
        all_fold_results.extend(fold_records)

    # Final model evaluation
    test_metrics = _evaluate_final_model(
        global_best_model, test_loader, device, logger
    )

    # Save global best model
    _save_global_best_model(global_best_model, args.fine_save)

    # Comprehensive results compilation
    all_results = {
        'global_best': {
            'loss': global_best_loss,
            'test_metrics': test_metrics,
            'model_details': _get_model_details(global_best_model)
        },
        'performance_history': run_performance_history,
        'run_stats': run_statistics,
        'all_folds': all_fold_results
    }

    return all_fold_results, all_results, global_best_model


# Helper Functions (to be implemented)
def set_all_seeds(seed):
    """Set all random seeds for reproducibility."""
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def _safe_model_initialization(model_template, device, run_idx, fold_idx):
    """Safely initialize model with error handling."""
    model = copy.deepcopy(model_template).to(device)
    model.train()
    return model


def _create_data_loaders(dataset, train_idx, val_idx, args):
    """Create train and validation data loaders."""
    train_loader = DataLoader(
        Subset(dataset, train_idx),
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers
    )
    val_loader = DataLoader(
        Subset(dataset, val_idx),
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers
    )
    return train_loader, val_loader


def _evaluate_fold(model, val_loader, device):
    """Evaluate model on validation set."""
    val_metrics = evaluate_model_2(
        model,
        lambda pred, target: model.compute_loss(pred, target),
        val_loader,
        device
    )
    return val_metrics


def _save_run_best_model(model, save_dir, run_idx):
    """Safely save run's best model."""
    if model is not None and isinstance(model, torch.nn.Module):
        save_path = os.path.join(
            save_dir,
            f"transfer_model_best_run_{run_idx + 1}.pth"
        )
        torch.save(model.state_dict(), save_path)
        print(f"[ä¿å­˜] Current Run {run_idx + 1} Best Model => {save_path}")



def predict_unlabeled_data(
    new_data_path,
    new_data_sheet,
    metal_properties_sheet,
    solvent_properties_sheet,
    model_checkpoint_path,
    pca_model_path,
    args,
    device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
):
    """
    ä½¿ç”¨å·²è®­ç»ƒ/è¿ç§»å­¦ä¹ å¥½çš„ GNN æ¨¡å‹ï¼Œå¯¹æ— æ ‡ç­¾æ–°æ•°æ®è¿›è¡Œæ¨ç†é¢„æµ‹ï¼Œå¹¶å¤„ç† group_id åˆ—ã€‚

    - å¦‚æœæ–°æ•°æ®ä¸­æ—  group_id åˆ—ï¼Œåˆ™è‡ªåŠ¨æŒ‰ [SMILES, Solvent1, Solvent2, Solvent3, Solvent4] è¿›è¡Œåˆ†ç»„å¹¶åˆ†é….
    - å¦‚æœå·²æœ‰ group_id åˆ—ï¼Œåˆ™ä¿ç•™åŸæ ·.
    """

    # 1) è¯»å–æ–°æ•°æ®
    new_df = pd.read_excel(new_data_path, sheet_name=new_data_sheet)

    # 2) ç¡®è®¤å¹¶ç”Ÿæˆ/ä¿ç•™ group_id
    if "group_id" not in new_df.columns:
        print("No 'group_id' column found in new data. Generating via assign_group_id_by_solvents...")
        new_df = assign_group_id_by_solvents(new_df)
    else:
        print("Detected 'group_id' in new data. Keeping it for reference.")

    # 3) åˆå§‹åŒ–ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„å¤„ç†å™¨å¹¶åŠ è½½PCAæ¨¡å‹
    new_data_processor = MolecularDataProcessor(n_components=50)
    pca_file = os.path.join(pca_model_path, f"{args.time}_{args.record}.pkl")
    new_data_processor.load_pca_model(pca_file)

    # 4) è®¡ç®—æŒ‡çº¹å¹¶è¿›è¡ŒPCAå˜æ¢
    new_fingerprints_raw = new_data_processor.compute_fingerprints(new_df)
    new_fingerprints = new_data_processor.transform_fingerprints(new_fingerprints_raw)

    # 5) æ„å»ºDataset
    unlabeled_dataset = process_data_with_fingerprints(
        new_df,
        metal_properties_sheet,
        solvent_properties_sheet,
        new_fingerprints,
        augment=False,
        data_split='test'
    )

    # 6) DataLoader
    unlabeled_loader = DataLoader(
        unlabeled_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        pin_memory=False,
        persistent_workers=args.num_workers > 0
    )

    # 7) åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    model = GNN(
        node_features=unlabeled_dataset[0].x.size(1),
        edge_features=unlabeled_dataset[0].edge_attr.size(1),
        metal_features=unlabeled_dataset[0].metal_feats.size(1),
        solvent_features=36,  # ä¸åŸè®­ç»ƒæ—¶ä¸€è‡´
        hidden_dim=args.hidden_dim,
        num_classes=4,        # ä¸åŸè®­ç»ƒæ—¶ä¸€è‡´
        dropout_ratio=args.dropout_ratio
    ).to(device)

    # åŠ è½½æ¯ä¸ªæŠ˜çš„æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†
    all_preds = []
    # å‡è®¾æˆ‘ä»¬æœ‰å¤šä¸ªæ¨¡å‹ï¼Œä¸‹é¢ä»…åŠ è½½å•ä¸ªæ¨¡å‹ä½œä¸ºå‚è€ƒã€‚
    for checkpoint_path in model_checkpoint_path:  # å‡è®¾è¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªæ¨¡å‹è·¯å¾„çš„åˆ—è¡¨
        state_dict = torch.load(checkpoint_path, map_location='cpu')
        model.load_state_dict(state_dict)
        model.eval()

        with torch.no_grad():
            for batch_data in unlabeled_loader:
                batch_data = batch_data.to(device)
                outputs = model(batch_data)
                preds = outputs.argmax(dim=1).detach().cpu().numpy()
                all_preds.extend(preds)

    # 8) åˆå¹¶é¢„æµ‹ç»“æœï¼Œå¹¶ä¿ç•™ group_id
    result_df = new_df.copy()
    # å¦‚æœæœ‰å¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œå¯ä»¥è¿›è¡Œå¹³å‡æˆ–æŠ•ç¥¨ç­‰è¿›ä¸€æ­¥å¤„ç†
    result_df["prediction"] = all_preds

    # ç»“æœä¸­å°±åŒ…å«äº† group_idã€prediction ä»¥åŠæ–°æ•°æ®çš„åŸå§‹åˆ—
    return result_df


def stratified_sample_indices_imbalanced(labels: np.ndarray, per_class_config: dict = None, seed: int = 0):
    """
    é’ˆå¯¹ä¸¥é‡ä¸å¹³è¡¡æ•°æ®çš„åˆ†å±‚é‡‡æ ·å‡½æ•°

    Parameters:
    -----------
    labels : np.ndarray
        æ ‡ç­¾æ•°ç»„
    per_class_config : dict
        æ¯ä¸ªç±»åˆ«çš„é‡‡æ ·é…ç½®ï¼Œä¾‹å¦‚ {0: 350, 1: 180, 2: 32, 3: 8}
        å¦‚æœä¸ºNoneï¼Œä¼šæ ¹æ®æ‚¨çš„æ•°æ®åˆ†å¸ƒè‡ªåŠ¨è®¾ç½®
    seed : int
        éšæœºç§å­
    """

    # é»˜è®¤é…ç½®ï¼šæ ¹æ®æ‚¨çš„å®é™…æ•°æ®åˆ†å¸ƒè®¾è®¡
    if per_class_config is None:
        per_class_config = {
            0: 350,  # ä»3815ä¸­é‡‡æ ·350ï¼Œçº¦9.2%
            1: 180,  # ä»1133ä¸­é‡‡æ ·180ï¼Œçº¦15.9%
            2: 32,  # å…¨éƒ¨ä½¿ç”¨ï¼ˆç¨€æœ‰ç±»ï¼‰
            3: 8  # å…¨éƒ¨ä½¿ç”¨ï¼ˆæç¨€æœ‰ç±»ï¼‰
        }

    rng = np.random.RandomState(seed)
    idx_list = []

    print("ğŸ¯ æ‰§è¡Œä¸å¹³è¡¡æ•°æ®åˆ†å±‚é‡‡æ ·...")
    print("-" * 50)

    for cls in np.unique(labels):
        cls_idx = np.where(labels == cls)[0]

        # è·å–è¯¥ç±»åˆ«çš„é‡‡æ ·é…ç½®
        if cls in per_class_config:
            per_class = per_class_config[cls]
        else:
            per_class = min(100, len(cls_idx))  # é»˜è®¤ç­–ç•¥

        if len(cls_idx) <= per_class:
            # ç¨€æœ‰ç±»å…¨éƒ¨ä¿ç•™
            take = cls_idx
            strategy = "å…¨éƒ¨ä½¿ç”¨(ç¨€æœ‰ç±»)"
        else:
            # å……è¶³ç±»åˆ«è¿›è¡Œé‡‡æ ·
            take = rng.choice(cls_idx, per_class, replace=False)
            strategy = f"éšæœºé‡‡æ ·({per_class}/{len(cls_idx)} = {per_class / len(cls_idx):.1%})"

        idx_list.extend(take.tolist())

        print(f"  ç±»åˆ«{cls}: {len(take)} ä¸ªæ ·æœ¬ - {strategy}")

    rng.shuffle(idx_list)

    print(f"\nğŸ“Š é‡‡æ ·å®Œæˆ: æ€»è®¡ {len(idx_list)} ä¸ªæ ·æœ¬")
    sampled_labels = labels[idx_list]
    print(f"é‡‡æ ·ååˆ†å¸ƒ: {dict(Counter(sampled_labels))}")
    print("-" * 50)

    return np.array(idx_list, dtype=np.int64)



def extract_model_latent_representations_enhanced(
        model,
        data_loader,
        device,
        model_label,
        max_samples=None,  # æ”¹ä¸ºNoneï¼Œæå–æ‰€æœ‰æ•°æ®
        verbose=True
):
    """
    å¢å¼ºç‰ˆlatent representationsæå–ï¼ˆæå–æ›´å¤šæ•°æ®ï¼‰
    """
    model.eval()
    latent_representations = []
    sample_count = 0
    total_batches = len(data_loader)

    # Hookæœºåˆ¶
    intermediate_outputs = {}

    def get_activation(name):
        def hook(model, input, output):
            if isinstance(output, tuple):
                intermediate_outputs[name] = output[0].detach().cpu()
            else:
                intermediate_outputs[name] = output.detach().cpu()

        return hook

    hooks = []

    # æ³¨å†Œhooks
    if hasattr(model, 'fusion_layer'):
        hook = model.fusion_layer.register_forward_hook(get_activation('fusion_output'))
        hooks.append(hook)
    elif hasattr(model, 'output_layer'):
        hook = model.output_layer.register_forward_hook(get_activation('pre_output'))
        hooks.append(hook)

    if verbose:
        print(f"ğŸ”„ æå– {model_label} æ¨¡å‹çš„ latent representations...")
        print(f"   æ•°æ®åŠ è½½å™¨æ€»batchæ•°: {total_batches}")

    try:
        with torch.no_grad():
            for batch_idx, batch in enumerate(data_loader):
                if max_samples and sample_count >= max_samples:
                    break

                batch = batch.to(device)
                intermediate_outputs.clear()

                # å‰å‘ä¼ æ’­
                _ = model(batch)

                # æå–è¡¨ç¤º
                batch_representations = None
                if 'fusion_output' in intermediate_outputs:
                    batch_representations = intermediate_outputs['fusion_output']
                    if batch_representations.shape[1] == model.hidden_dim * 2:
                        batch_representations = batch_representations[:, :model.hidden_dim]
                elif 'pre_output' in intermediate_outputs:
                    batch_representations = intermediate_outputs['pre_output']
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆ
                    batch_representations = extract_features_manually_enhanced(model, batch)

                if batch_representations is not None:
                    latent_representations.append(batch_representations.numpy())
                    sample_count += len(batch_representations)

                if verbose and (batch_idx + 1) % 5 == 0:
                    print(f"   å·²å¤„ç† {batch_idx + 1}/{total_batches} ä¸ªbatchï¼Œç´¯è®¡æ ·æœ¬: {sample_count}")

    finally:
        for hook in hooks:
            hook.remove()

    if latent_representations:
        all_representations = np.vstack(latent_representations)
        if verbose:
            print(f"âœ… æˆåŠŸæå– {len(all_representations)} ä¸ª {model_label} æ ·æœ¬çš„è¡¨ç¤º")
            print(f"   è¡¨ç¤ºç»´åº¦: {all_representations.shape[1]}")
        return all_representations
    else:
        if verbose:
            print(f"âŒ {model_label} æœªæå–åˆ°æœ‰æ•ˆè¡¨ç¤º")
        return None


def extract_features_manually_enhanced(model, batch):
    """å¢å¼ºç‰ˆæ‰‹åŠ¨ç‰¹å¾æå–"""
    try:
        x, edge_index, batch_tensor = batch.x, batch.edge_index, batch.batch

        # å›¾ç‰¹å¾ç¼–ç 
        graph_features = model.encoder(x, edge_index, batch_tensor)

        # é‡‘å±ç‰¹å¾å¤„ç†
        metal_feats = torch.nn.functional.dropout(
            torch.nn.functional.relu(model.metal_fc(batch.metal_feats))
        )

        # æº¶å‰‚æ³¨æ„åŠ›
        solvent_feats = model.solvent_attention(
            batch.solvent1_feats, batch.solvent2_feats,
            batch.solvent3_feats, batch.solvent4_feats
        )

        # ç‰¹å¾äº¤äº’
        interaction_features = graph_features * metal_feats

        # ç»„åˆç‰¹å¾
        combined_features = torch.cat([
            graph_features,
            metal_feats * model.metal_weight,
            solvent_feats,
            interaction_features
        ], dim=1)

        # èåˆå±‚
        fused_features = model.fusion_layer(combined_features)

        # é—¨æ§æœºåˆ¶
        main_path, gate_path = torch.split(fused_features, [model.hidden_dim, model.hidden_dim], dim=1)
        gate = model.gate_layer(gate_path)
        gated_features = main_path * gate

        return gated_features.detach().cpu()

    except Exception as e:
        print(f"æ‰‹åŠ¨ç‰¹å¾æå–å¤±è´¥: {e}")
        return None


import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.decomposition import PCA
from scipy.stats import gaussian_kde

# ==================== å…¨å±€å­—ä½“è®¾ç½® ====================
mpl.rcParams['font.family'] = 'Times New Roman'
mpl.rcParams['font.size'] = 13
mpl.rcParams['axes.labelweight'] = 'bold'
mpl.rcParams['axes.titleweight'] = 'bold'
mpl.rcParams['legend.fontsize'] = 12
mpl.rcParams['axes.titlesize'] = 15
mpl.rcParams['axes.labelsize'] = 14


def create_enhanced_latent_space_comparison(
        pretrained_model_path: str,
        finetuned_model_path: str,
        data_loader,
        model_template,
        device,
        output_dir: str,
        filename_prefix: str = "enhanced_latent_comparison",
        use_all_data: bool = True,
        random_state: int = 42
):
    """
    åˆ›å»ºå¢å¼ºç‰ˆ PCA latent space å¯¹æ¯”ï¼ˆä¸¤ä¸ªå›¾å•ç‹¬è¾“å‡ºï¼Œè®ºæ–‡çº§å¯è§†åŒ–ï¼‰
    """

    os.makedirs(output_dir, exist_ok=True)

    print("ğŸ“‚ åŠ è½½æ¨¡å‹å¹¶æå– latent representations...")
    print(f"   ä½¿ç”¨æ‰€æœ‰æ•°æ®: {use_all_data}")

    # ==================== åˆ›å»ºæ¨¡å‹å®ä¾‹ ====================
    def create_model_instance():
        return type(model_template)(
            node_features=model_template.encoder.convs[0].in_channels
            if hasattr(model_template.encoder, 'convs') else 128,
            edge_features=3,
            metal_features=model_template.metal_fc[0].in_features
            if hasattr(model_template, 'metal_fc') else 64,
            solvent_features=36,
            hidden_dim=model_template.hidden_dim,
            num_classes=4,
            dropout_ratio=0.1
        ).to(device)

    print("   åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    pretrained_model = create_model_instance()
    pretrained_model.load_state_dict(
        torch.load(pretrained_model_path, map_location=device),
        strict=False
    )

    print("   åŠ è½½å¾®è°ƒæ¨¡å‹...")
    finetuned_model = create_model_instance()
    finetuned_model.load_state_dict(
        torch.load(finetuned_model_path, map_location=device),
        strict=False
    )

    # ==================== æå–è¡¨ç¤º ====================
    max_samples = None if use_all_data else 1000

    pretrained_representations = extract_model_latent_representations_enhanced(
        pretrained_model, data_loader, device,
        model_label="Pretrained", max_samples=max_samples, verbose=True
    )

    finetuned_representations = extract_model_latent_representations_enhanced(
        finetuned_model, data_loader, device,
        model_label="Finetuned", max_samples=max_samples, verbose=True
    )

    if pretrained_representations is None or finetuned_representations is None:
        print("âŒ è¡¨ç¤ºæå–å¤±è´¥")
        return None

    # ==================== å¯¹é½æ ·æœ¬æ•° ====================
    min_samples = min(len(pretrained_representations), len(finetuned_representations))
    pretrained_representations = pretrained_representations[:min_samples]
    finetuned_representations = finetuned_representations[:min_samples]

    print(f"ğŸ“Š æ ·æœ¬ç»Ÿè®¡: {min_samples}")

    # ==================== PCA ====================
    print("ğŸ”„ æ‰§è¡Œ PCA é™ç»´...")
    all_representations = np.vstack([pretrained_representations, finetuned_representations])
    pca = PCA(n_components=2, random_state=random_state)
    all_pca = pca.fit_transform(all_representations)

    pretrained_pca = all_pca[:min_samples]
    finetuned_pca = all_pca[min_samples:]

    print(f"ğŸ“Š PCA æ–¹å·®è§£é‡Šç‡: {pca.explained_variance_ratio_.sum():.3f}")

    # ==================== å›¾ 1ï¼šæ•£ç‚¹å›¾ ====================
    print("ğŸ¨ ç»˜åˆ¶ PCA æ•£ç‚¹å›¾...")
    plt.style.use('seaborn-v0_8-whitegrid')
    fig1, ax1 = plt.subplots(figsize=(8, 7))

    ax1.scatter(
        pretrained_pca[:, 0], pretrained_pca[:, 1],
        c='#FF6B6B', s=60, alpha=0.7,
        edgecolors='white', linewidths=0.6,
        label='Finetuned (with Pretrain)'
    )

    ax1.scatter(
        finetuned_pca[:, 0], finetuned_pca[:, 1],
        c='#4ECDC4', s=60, alpha=0.7,
        edgecolors='white', linewidths=0.6,
        label='Trained (without Pretrain)'
    )

    ax1.set_xlabel('PCA Dimension 1')
    ax1.set_ylabel('PCA Dimension 2')
    ax1.set_title('Latent Space Comparison (PCA)')
    ax1.legend()
    ax1.grid(alpha=0.3)

    plt.tight_layout()
    scatter_path = os.path.join(output_dir, f"{filename_prefix}_scatter.png")
    plt.savefig(scatter_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()

    # ==================== å›¾ 2ï¼šå¯†åº¦å›¾ ====================
    print("ğŸ¨ ç»˜åˆ¶ PCA å¯†åº¦åˆ†å¸ƒå›¾...")
    fig2, ax2 = plt.subplots(figsize=(8, 7))

    all_x = np.concatenate([pretrained_pca[:, 0], finetuned_pca[:, 0]])
    all_y = np.concatenate([pretrained_pca[:, 1], finetuned_pca[:, 1]])

    xx, yy = np.mgrid[
        all_x.min()-1:all_x.max()+1:0.1,
        all_y.min()-1:all_y.max()+1:0.1
    ]
    positions = np.vstack([xx.ravel(), yy.ravel()])

    if len(pretrained_pca) > 1:
        kde1 = gaussian_kde(pretrained_pca.T)
        ax2.contour(xx, yy, kde1(positions).reshape(xx.shape),
                    colors='#FF6B6B', levels=6, linewidths=1.5)

    if len(finetuned_pca) > 1:
        kde2 = gaussian_kde(finetuned_pca.T)
        ax2.contour(xx, yy, kde2(positions).reshape(xx.shape),
                    colors='#4ECDC4', levels=6, linewidths=1.5)

    ax2.scatter(pretrained_pca[:, 0], pretrained_pca[:, 1],
                c='#FF6B6B', alpha=0.4, s=30,
                label='Finetuned (with Pretrain)')
    ax2.scatter(finetuned_pca[:, 0], finetuned_pca[:, 1],
                c='#4ECDC4', alpha=0.4, s=30,
                label='Trained (without Pretrain)')

    ax2.set_xlabel('PCA Dimension 1')
    ax2.set_ylabel('PCA Dimension 2')
    ax2.set_title('Latent Space Density Distribution')
    ax2.legend()
    ax2.grid(alpha=0.3)

    plt.tight_layout()
    density_path = os.path.join(output_dir, f"{filename_prefix}_density.png")
    plt.savefig(density_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()

    # ==================== Excel ====================
    print("ğŸ“Š å¯¼å‡º Excel æ•°æ®...")
    excel_path = os.path.join(output_dir, f"{filename_prefix}_data.xlsx")

    df = pd.DataFrame({
        'PCA_1': np.concatenate([pretrained_pca[:, 0], finetuned_pca[:, 0]]),
        'PCA_2': np.concatenate([pretrained_pca[:, 1], finetuned_pca[:, 1]]),
        'Model': ['Finetuned (with Pretrain)'] * min_samples +
                 ['Trained (without Pretrain)'] * min_samples
    })

    df.to_excel(excel_path, index=False)

    print("âœ… ç»“æœå·²ä¿å­˜")
    print(f"   ğŸ“Š Excel: {excel_path}")
    print(f"   ğŸ¨ Scatter: {scatter_path}")
    print(f"   ğŸ¨ Density: {density_path}")

    return {
        'total_samples': min_samples,
        'pca_explained_variance': pca.explained_variance_ratio_.tolist(),
        'files': {
            'excel': excel_path,
            'scatter_plot': scatter_path,
            'density_plot': density_path
        }
    }

def run_fair_transfer_learning_comparison(args, transfer_data_loaders, base_model, device, t):
    """
    å…¬å¹³çš„è¿ç§»å­¦ä¹ å¯¹æ¯”ï¼šä¸¤ä¸ªæ¨¡å‹éƒ½ä½¿ç”¨ç›¸åŒçš„äº¤å‰éªŒè¯è®­ç»ƒæ–¹æ³•
    """

    print("\nğŸ¯ å…¬å¹³çš„è¿ç§»å­¦ä¹ å¯¹æ¯”å®éªŒ")
    print("   ğŸ”´ å®éªŒç»„: é¢„è®­ç»ƒ+å¾®è°ƒæ¨¡å‹ (ä½¿ç”¨kf_cross_validate)")
    print("   ğŸ”µ å¯¹ç…§ç»„: ä»å¤´è®­ç»ƒæ¨¡å‹ (ä½¿ç”¨kf_cross_validate)")
    print("   ğŸ“Š è®­ç»ƒæ–¹æ³•: å®Œå…¨ç›¸åŒï¼Œåªæœ‰åˆå§‹æƒé‡ä¸åŒ")

    # ================== å‡†å¤‡ç›¸åŒçš„æ•°æ®é›† ==================
    # åˆå¹¶trainå’Œvalä½œä¸ºfull datasetï¼ˆå’Œé¢„è®­ç»ƒ+å¾®è°ƒæ¨¡å‹ä¸€æ ·ï¼‰
    full_train_val = transfer_data_loaders["train"].dataset + transfer_data_loaders["val"].dataset
    test_loader = transfer_data_loaders["test"]

    print(f"ğŸ“Š æ•°æ®å‡†å¤‡:")
    print(f"   è®­ç»ƒ+éªŒè¯æ•°æ®: {len(full_train_val)} æ ·æœ¬")
    print(f"   æµ‹è¯•æ•°æ®: {sum(len(batch.y) for batch in test_loader)} æ ·æœ¬")

    # ================== è·¯å¾„è®¾ç½® ==================
    pretrained_finetuned_path = f"{args.fine_save}/{args.count}_{t}.pth"
    from_scratch_path = f"{args.fine_save}/kf_from_scratch_{args.count}_{t}.pth"

    # ================== è®­ç»ƒé¢„è®­ç»ƒ+å¾®è°ƒæ¨¡å‹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰==================
    if not os.path.exists(pretrained_finetuned_path):
        print("\nğŸ”„ è®­ç»ƒé¢„è®­ç»ƒ+å¾®è°ƒæ¨¡å‹ï¼ˆä½¿ç”¨äº¤å‰éªŒè¯ï¼‰...")

        try:
            fold_results_pretrained, all_results_pretrained, best_model_pretrained = kf_cross_validate_transfer_learning(
                model_template=base_model,  # ğŸ”´ å·²åŠ è½½é¢„è®­ç»ƒæƒé‡
                full_dataset=full_train_val,
                test_loader=test_loader,
                args=args,
                device=device
            )

            torch.save(best_model_pretrained.state_dict(), pretrained_finetuned_path)
            print(f"âœ… é¢„è®­ç»ƒ+å¾®è°ƒæ¨¡å‹å·²ä¿å­˜: {pretrained_finetuned_path}")

        except Exception as e:
            print(f"âŒ é¢„è®­ç»ƒ+å¾®è°ƒè®­ç»ƒå¤±è´¥: {e}")
            return None
    else:
        print(f"âœ… é¢„è®­ç»ƒ+å¾®è°ƒæ¨¡å‹å·²å­˜åœ¨: {pretrained_finetuned_path}")

    # ================== è®­ç»ƒä»å¤´å¼€å§‹æ¨¡å‹ï¼ˆä½¿ç”¨ç›¸åŒæ–¹æ³•ï¼‰==================
    if not os.path.exists(from_scratch_path):
        print("\nğŸ”„ è®­ç»ƒä»å¤´å¼€å§‹æ¨¡å‹ï¼ˆä½¿ç”¨ç›¸åŒçš„äº¤å‰éªŒè¯ï¼‰...")

        try:
            # åˆ›å»ºä¸åŠ è½½é¢„è®­ç»ƒæƒé‡çš„æ¨¡æ¿æ¨¡å‹
            sample_data = next(iter(transfer_data_loaders["train"]))
            from_scratch_template = GNN(
                node_features=sample_data.x.size(1),
                edge_features=sample_data.edge_attr.size(1),
                metal_features=sample_data.metal_feats.size(1),
                solvent_features=36,
                hidden_dim=args.hidden_dim,
                num_classes=4,
                dropout_ratio=args.dropout_ratio
            ).to(device)

            # ğŸš¨ å…³é”®ï¼šä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ä¿æŒéšæœºåˆå§‹åŒ–
            print("ğŸ”µ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è¿›è¡Œäº¤å‰éªŒè¯è®­ç»ƒ...")

            fold_results_scratch, all_results_scratch, best_model_scratch = kf_cross_validate_transfer_learning(
                model_template=from_scratch_template,  # ğŸ”µ éšæœºåˆå§‹åŒ–æ¨¡å‹
                full_dataset=full_train_val,  # ğŸ”„ ç›¸åŒæ•°æ®
                test_loader=test_loader,  # ğŸ”„ ç›¸åŒæµ‹è¯•é›†
                args=args,  # ğŸ”„ ç›¸åŒè¶…å‚æ•°
                device=device  # ğŸ”„ ç›¸åŒè®¾å¤‡
            )

            torch.save(best_model_scratch.state_dict(), from_scratch_path)
            print(f"âœ… ä»å¤´è®­ç»ƒæ¨¡å‹å·²ä¿å­˜: {from_scratch_path}")

            # æ‰“å°å¯¹æ¯”ç»“æœ
            print(f"\nğŸ“Š è®­ç»ƒç»“æœå¯¹æ¯”:")
            if 'fold_results_pretrained' in locals():
                print(f"   ğŸ”´ é¢„è®­ç»ƒ+å¾®è°ƒ: {fold_results_pretrained}")
            if 'fold_results_scratch' in locals():
                print(f"   ğŸ”µ ä»å¤´è®­ç»ƒ: {fold_results_scratch}")

        except Exception as e:
            print(f"âŒ ä»å¤´è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    else:
        print(f"âœ… ä»å¤´è®­ç»ƒæ¨¡å‹å·²å­˜åœ¨: {from_scratch_path}")

    # ================== Latent Spaceå¯¹æ¯”ï¼ˆä½¿ç”¨æµ‹è¯•é›†ï¼‰==================
    print(f"\nğŸ”¬ ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œå…¬å¹³çš„Latent Spaceå¯¹æ¯”...")

    try:
        comparison_results = create_enhanced_latent_space_comparison(
            pretrained_model_path=pretrained_finetuned_path,  # ğŸ”´ é¢„è®­ç»ƒ+å¾®è°ƒ(kfè®­ç»ƒ)
            finetuned_model_path=from_scratch_path,  # ğŸ”µ ä»å¤´è®­ç»ƒ(kfè®­ç»ƒ)
            data_loader=test_loader,  # ğŸ“Š æµ‹è¯•é›†
            model_template=base_model,
            device=device,
            output_dir=f"{args.output_dir}/fair_kf_comparison",
            filename_prefix=f"fair_kf_pretrain_vs_scratch_{args.time}_{args.record}_{args.count}_{t}",
            use_all_data=True,
            random_state=42
        )

        if comparison_results:
            print("âœ… å…¬å¹³çš„Latent Spaceå¯¹æ¯”å®Œæˆï¼")
            print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {args.output_dir}/fair_kf_comparison/")
            print("ğŸ¯ ä¸¤ä¸ªæ¨¡å‹ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼Œåªæœ‰åˆå§‹æƒé‡ä¸åŒï¼")

            return {
                'pretrained_model_path': pretrained_finetuned_path,
                'from_scratch_model_path': from_scratch_path,
                'training_method': 'k-foldäº¤å‰éªŒè¯',
                'comparison_results': comparison_results
            }

    except Exception as e:
        print(f"âŒ å¯¹æ¯”åˆ†æå‡ºé”™: {e}")
        return None

    return None
def load_model_checkpoint(model, checkpoint_path, device, strict=True):
    """
    åŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæ”¯æŒä¸¥æ ¼/éä¸¥æ ¼ï¼‰
    """
    if not checkpoint_path or not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

    print(f"Loading model parameters from: {checkpoint_path}")
    state_dict = torch.load(checkpoint_path, map_location=device)

    missing_keys, unexpected_keys = model.load_state_dict(
        state_dict, strict=strict
    )

    if not strict:
        print("âš ï¸ Non-strict loading enabled")
        if missing_keys:
            print("Missing keys:")
            for k in missing_keys:
                print(f"  - {k}")
        if unexpected_keys:
            print("Unexpected keys:")
            for k in unexpected_keys:
                print(f"  - {k}")

    print("âœ… Model parameters loaded successfully.")
    return model

def main():
    parser = argparse.ArgumentParser(description="GNN for property prediction")
    parser.add_argument("--data_path", type=str, default="/home/qyl/CSD_data/1_CSD/DATA_clean/output_deduped2.xlsx")
    parser.add_argument("--sheet", type=str, default="sol_min5")
    parser.add_argument("--source_batch_size", type=int, default=128)
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--epochs", type=int, default=1000)
    parser.add_argument("--lr", type=float, default=0.0001)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--hidden_dim1", type=int, default=128)
    parser.add_argument("--hidden_dim2", type=int, default=128)
    parser.add_argument("--hidden_dim3", type=int, default=128)
    parser.add_argument("--dropout_ratio", type=float, default=0.5)
    parser.add_argument("--num_heads", type=int, default=4)
    parser.add_argument("--transfer", type=str, default=True)
    parser.add_argument("--transfer_sheet", type=str, default="unlabbeled2")
    parser.add_argument("--transfer_data_path", type=str,
                        default="/home/qyl/CSD_data/1_CSD/DATA_clean/output_deduped2.xlsx")
    parser.add_argument("--tran_save", type=str, default="/home/qyl/CSD_data/1_CSD/logs/ensemble/4.10____4")
    parser.add_argument("--fine_save", type=str, default="/home/qyl/CSD_data/1_CSD/logs/ensemble/4.10____4")
    parser.add_argument("--output_dir", type=str, default="/home/qyl/CSD_data/1_CSD/DATA_clean/data_supp_going_rf")
    parser.add_argument("--time", type=str, default="sol_min4_FF60")
    parser.add_argument("--ori_arg", type=str, default=False)
    parser.add_argument("--tran_arg", type=str, default=False)
    parser.add_argument("--pca_model_path", type=str, default="/home/qyl/CSD_data/1_CSD/logs/ensemble/4.10____4")
    parser.add_argument("--hidden_dim", type=int, default=128)
    parser.add_argument("--num_workers", type=int, default=0)
    parser.add_argument("--record", type=str, default="5")
    parser.add_argument("--count", type=str, default="19111111111")
    parser.add_argument("--cc", type=str, default="333")
    parser.add_argument("--seed", type=int, default=42)

    args = parser.parse_args()
    num_classes = 4

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    seed = 42

    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    cache_manager = DataCacheManager(cache_dir=f"{args.tran_save}/data_cache")

    # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    df = pd.read_excel(args.data_path, sheet_name=args.sheet)
    metal_properties_sheet = pd.read_excel(args.data_path, 'metal')
    solvent_properties_sheet = pd.read_excel(args.data_path, 'sol_pro4')

    # ==================== æ³¨é‡Šçš„æºåŸŸè®­ç»ƒéƒ¨åˆ† ====================

    print(f"{args.time}:", args.time)

    data_loaders, cache_manager, origin_data = create_data_loaders_with_cache(
        args, df, metal_properties_sheet, solvent_properties_sheet,
        cache_manager, force_reload=False
    )

    train_loader = data_loaders["train"]
    val_loader = data_loaders["val"]
    test_loader = data_loaders["test"]

    # 5. æ¨¡å‹åˆå§‹åŒ–
    model = GNN(
        node_features=origin_data["train"][0].x.size(1),
        edge_features=origin_data["train"][0].edge_attr.size(1),
        metal_features=origin_data["train"][0].metal_feats.size(1),
        solvent_features=36,
        hidden_dim=args.hidden_dim,
        num_classes=num_classes,
        dropout_ratio=args.dropout_ratio
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.75, patience=15, verbose=True
    )

    trained_model, best_train_results, best_val_results, best_test_results, _, _, _ = train_and_evaluate(
        model,
        optimizer,
        scheduler,
        lambda pred, target: model.compute_loss(pred, target),
        data_loaders,
        args.epochs,
        device,
        title="Origin_"
    )

    save_results_to_excel(
        best_train_results, best_val_results, best_test_results,
        args.output_dir,
        f"source_domain_results{args.time}_seed{seed}_{args.record}_{args.count}.xlsx"
    )

    torch.save(
        model.state_dict(),
        f"{args.tran_save}/origin_model_state{args.time}_seed{seed}_{args.record}_{args.count}.pth"
    )

    # ==================== Transfer Learning éƒ¨åˆ† ====================
    if args.transfer:
        # ================== Target Domain Data Preparation ==================
        transfer_df = pd.read_excel(args.transfer_data_path, sheet_name=args.transfer_sheet)
        transfer_data_processor = MolecularDataProcessor(n_components=50)
        transfer_data_processor.load_pca_model(f"{args.pca_model_path}/{args.time}_{args.record}.pkl")

        # t = random.randint(1, 10000)
        # print("Random seed for transfer data:", t)
        t = 42
        # Prepare target domain datasets
        (transfer_train_df, transfer_val_df, transfer_test_df), fingerprint_data = prepare_datasets(
            transfer_df, transfer_data_processor, test_size=0.4, random_state=t, is_training=False
        )

        augment = False
        print("Augment:", augment)

        # Process data with fingerprints
        transfer_train = process_data_with_fingerprints(
            transfer_train_df, metal_properties_sheet, solvent_properties_sheet,
            fingerprint_data[0], augment=augment, data_split='train'
        )
        transfer_val = process_data_with_fingerprints(
            transfer_val_df, metal_properties_sheet, solvent_properties_sheet,
            fingerprint_data[1], augment=False, data_split='val'
        )
        transfer_test = process_data_with_fingerprints(
            transfer_test_df, metal_properties_sheet, solvent_properties_sheet,
            fingerprint_data[2], augment=False, data_split='test'
        )

        # ================== Data Loaders Preparation ==================
        train_dataset = transfer_train
        if args.tran_arg:
            train_dataset = flexible_augment_data(
                transfer_train, max_augmentations=1, augmentation_probability=1
            )

        transfer_data_loaders = {
            "train": DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,
                                num_workers=args.num_workers, pin_memory=False,
                                persistent_workers=args.num_workers > 0),
            "val": DataLoader(transfer_val, batch_size=args.batch_size, shuffle=False,
                              num_workers=args.num_workers, pin_memory=False, persistent_workers=args.num_workers > 0),
            "test": DataLoader(transfer_test, batch_size=args.batch_size, shuffle=False,
                               num_workers=args.num_workers, pin_memory=False, persistent_workers=args.num_workers > 0)
        }

        # ================== Transfer Learning Model Setup ==================
        base_model = GNN(
            node_features=transfer_train[0].x.size(1),
            edge_features=transfer_train[0].edge_attr.size(1),
            metal_features=transfer_train[0].metal_feats.size(1),
            solvent_features=36,
            hidden_dim=args.hidden_dim,
            num_classes=num_classes,
            dropout_ratio=args.dropout_ratio
        ).to(device)

        state_dict = torch.load(f"{args.tran_save}/origin_model_state{args.time}_3.23_{args.record}.pth",
                                map_location='cpu')
        base_model.load_state_dict(state_dict, strict=False)

        # å†»ç»“ç‰¹å®šå±‚
        def freeze_transfer_layers(model):
            components_to_freeze = ['graph_encoder', 'metal_net', 'solvent_attn']
            model.freeze_components(components_to_freeze)

        freeze_transfer_layers(base_model)

        # ================== Enhanced Cross Validation ==================
        full_train_val = transfer_train + transfer_val
        print("\n=== Data Sanity Check ===")
        print(f"Full dataset size: {len(full_train_val)}")
        print(f"Test loader size: {len(transfer_data_loaders['test'].dataset)}")

        fold_results, all_results, best_model = kf_cross_validate_transfer_learning(
            model_template=base_model,
            full_dataset=full_train_val,
            test_loader=transfer_data_loaders["test"],
            args=args,
            device=device
        )

        # Save best model
        torch.save(
            best_model.state_dict(),
            f"{args.fine_save}/{args.count}_{t}.pth"
        )
        print(f"Saved best model to {args.fine_save}/{args.count}_{t}.pth")
        print("\nğŸš€ å¼€å§‹å…¬å¹³çš„è¿ç§»å­¦ä¹ å¯¹æ¯”ï¼ˆç›¸åŒäº¤å‰éªŒè¯æ–¹æ³•ï¼‰...")
        try:
            fair_comparison_results = run_fair_transfer_learning_comparison(
                args, transfer_data_loaders, base_model, device, t
            )

            if fair_comparison_results:
                print("âœ… å…¬å¹³å¯¹æ¯”å®Œæˆï¼")
                print("ğŸ¯ å…³é”®ä¼˜åŠ¿:")
                print("   - ä¸¤ä¸ªæ¨¡å‹ä½¿ç”¨ç›¸åŒçš„k-foldäº¤å‰éªŒè¯è®­ç»ƒ")
                print("   - ç›¸åŒçš„æ•°æ®åˆ†å‰²å’Œè¶…å‚æ•°")
                print("   - å”¯ä¸€å·®å¼‚ï¼šæ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡")
                print("   - ç»“æœæ›´ç§‘å­¦å¯ä¿¡ï¼")

        except Exception as e:
            print(f"âŒ å…¬å¹³å¯¹æ¯”å¤±è´¥: {e}")

        # ==================== åŸæœ‰çš„é¢„æµ‹éƒ¨åˆ† ====================
        # é¢„æµ‹æ–°æ•°æ®
        new_data_pred_df = predict_unlabeled_data(
            new_data_path="/home/qyl/CSD_data/1_CSD/DATA_clean/output_deduped2.xlsx",
            new_data_sheet="unlabbeled2",
            metal_properties_sheet=metal_properties_sheet,
            solvent_properties_sheet=solvent_properties_sheet,
            model_checkpoint_path=[
                f"{args.fine_save}/{args.count}_{t}.pth"
            ],  # åŒ…å«äº†è®­ç»ƒå¾—åˆ°çš„æœ€ä½³æ¨¡å‹
            pca_model_path=args.pca_model_path,
            args=args,
            device=device
        )

        # å°†ç»“æœå¦å­˜ä¸ºExcel
        output_path = f"/home/qyl/CSD_data/1_CSD/DATA_clean/pre_{args.count}_{t}_6_1.xlsx"
        new_data_pred_df.to_excel(output_path, index=False)
        print(f"é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {output_path}")

        # ä¿å­˜è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„ç»“æœ
        result_dict = {
            "Train": transfer_train_df.assign(prediction=None),  # æ·»åŠ ç©ºé¢„æµ‹åˆ—
            "Validation": transfer_val_df.assign(prediction=None),  # æ·»åŠ ç©ºé¢„æµ‹åˆ—
            "Test": transfer_test_df.assign(prediction=None)  # æ·»åŠ ç©ºé¢„æµ‹åˆ—
        }

        # å°†é¢„æµ‹ç»“æœï¼ˆè‹¥æœ‰çš„è¯ï¼‰å†™å…¥å¯¹åº”çš„ DataFrame ä¸­
        if 'prediction' in new_data_pred_df.columns:
            result_dict["Test"]["prediction"] = new_data_pred_df["prediction"]


            # å°†æ‰€æœ‰ç»“æœä¿å­˜åˆ°ä¸€ä¸ª Excel æ–‡ä»¶ä¸­
        with pd.ExcelWriter(f"/home/qyl/CSD_data/1_CSD/DATA_clean/results_{args.count}_{t}_6_1.xlsx") as writer:
            for name, result_df in result_dict.items():
                result_df.to_excel(writer, sheet_name=name, index=False)

        print(f"è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„ç»“æœå·²ä¿å­˜åˆ° /home/qyl/CSD_data/1_CSD/DATA_clean/results_{args.count}_{t}_6_1.xlsx")


if __name__ == "__main__":
    main()
